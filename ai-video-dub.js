import 'dotenv/config';
import OpenAI from 'openai';
import fs from 'fs';
import { exec, spawn } from 'child_process';
import { promisify } from 'util';
import { createInterface } from 'readline';

const execAsync = promisify(exec);

const rl = createInterface({
  input: process.stdin,
  output: process.stdout
});

const question = (query) => new Promise((resolve) => rl.question(query, resolve));

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

// Maximum characters per TTS request (OpenAI limit is 4096)
const MAX_TTS_CHARS = 4000;

// Maximum file size for Whisper API (25MB limit)
const MAX_WHISPER_FILE_SIZE = 24 * 1024 * 1024; // 24MB to be safe

// Chunk duration for splitting long audio files (in seconds)
const AUDIO_CHUNK_DURATION = 300; // 5 minutes per chunk

// Helper function to split text into chunks intelligently
function splitTextIntoChunks(text, maxChars = MAX_TTS_CHARS) {
  const chunks = [];
  
  // Split by sentences first
  const sentences = text.match(/[^.!?]+[.!?]+/g) || [text];
  
  let currentChunk = '';
  
  for (const sentence of sentences) {
    // If single sentence is too long, split by commas or spaces
    if (sentence.length > maxChars) {
      if (currentChunk) {
        chunks.push(currentChunk.trim());
        currentChunk = '';
      }
      
      // Split long sentence by commas
      const parts = sentence.split(',');
      for (const part of parts) {
        if ((currentChunk + part).length <= maxChars) {
          currentChunk += part + ',';
        } else {
          if (currentChunk) chunks.push(currentChunk.trim());
          currentChunk = part + ',';
        }
      }
    } else {
      // Try to add sentence to current chunk
      if ((currentChunk + sentence).length <= maxChars) {
        currentChunk += sentence;
      } else {
        if (currentChunk) chunks.push(currentChunk.trim());
        currentChunk = sentence;
      }
    }
  }
  
  if (currentChunk) {
    chunks.push(currentChunk.trim());
  }
  
  return chunks;
}

// Helper function to generate TTS for text chunks
async function generateTTSForChunks(text, voiceId, timestamp, targetDuration = null) {
  const chunks = splitTextIntoChunks(text);
  
  if (chunks.length === 1) {
    console.log('ğŸ“ Texto cabe em um Ãºnico chunk\n');
    return null; // Use normal single TTS
  }
  
  console.log(`ğŸ“ Texto dividido em ${chunks.length} chunks para processamento\n`);
  
  const audioChunks = [];
  
  for (let i = 0; i < chunks.length; i++) {
    console.log(`ğŸ”Š Gerando Ã¡udio para chunk ${i + 1}/${chunks.length}...`);
    console.log(`   Texto do chunk: ${chunks[i].length} caracteres`);
    
    const speechResponse = await openai.audio.speech.create({
      model: 'gpt-4o-mini-tts',
      voice: voiceId,
      input: chunks[i],
    });
    
    const buffer = Buffer.from(await speechResponse.arrayBuffer());
    const chunkFile = `temp_chunk_${timestamp}_${i}.mp3`;
    fs.writeFileSync(chunkFile, buffer);
    audioChunks.push(chunkFile);
    
    // Check duration of generated chunk
    const { stdout: chunkDurationInfo } = await execAsync(`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${chunkFile}"`);
    const chunkDuration = parseFloat(chunkDurationInfo.trim());
    
    console.log(`âœ… Chunk ${i + 1}/${chunks.length} gerado - ${chunkDuration.toFixed(2)}s de Ã¡udio`);
  }
  
  console.log('\nğŸ”— Concatenando todos os chunks de Ã¡udio...');
  
  // Verify all chunks exist and show their durations
  console.log('\nğŸ“Š Verificando chunks:');
  let totalChunkDuration = 0;
  for (let i = 0; i < audioChunks.length; i++) {
    const chunkFile = audioChunks[i];
    if (!fs.existsSync(chunkFile)) {
      console.error(`âŒ ERRO: Chunk ${i + 1} nÃ£o encontrado: ${chunkFile}`);
      throw new Error(`Chunk file missing: ${chunkFile}`);
    }
    
    const { stdout: chunkDurInfo } = await execAsync(`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${chunkFile}"`);
    const chunkDur = parseFloat(chunkDurInfo.trim());
    totalChunkDuration += chunkDur;
    console.log(`   Chunk ${i + 1}: ${chunkDur.toFixed(2)}s âœ“`);
  }
  console.log(`   Total esperado: ${totalChunkDuration.toFixed(2)}s\n`);
  
  // Create a file list for ffmpeg concat
  const concatListFile = `temp_concat_${timestamp}.txt`;
  const concatList = audioChunks.map(file => `file '${file}'`).join('\n');
  fs.writeFileSync(concatListFile, concatList);
  
  const finalAudioFile = `dubbed_audio_${timestamp}.mp3`;
  
  // Concatenate all audio chunks
  await execAsync(`ffmpeg -f concat -safe 0 -i "${concatListFile}" -c copy "${finalAudioFile}" -y`);
  
  // Verify final concatenated duration
  const { stdout: finalDurInfo } = await execAsync(`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${finalAudioFile}"`);
  const finalDur = parseFloat(finalDurInfo.trim());
  
  console.log('âœ… Ãudio completo concatenado');
  console.log(`   DuraÃ§Ã£o final: ${finalDur.toFixed(2)}s`);
  
  if (Math.abs(finalDur - totalChunkDuration) > 1) {
    console.log(`   âš ï¸  AVISO: DiferenÃ§a de ${Math.abs(finalDur - totalChunkDuration).toFixed(2)}s entre chunks e arquivo final!`);
  }
  console.log('');
  
  // If target duration is specified, stretch audio to match
  if (targetDuration) {
    const { stdout: currentDurationInfo } = await execAsync(`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${finalAudioFile}"`);
    const currentDuration = parseFloat(currentDurationInfo.trim());
    const stretchRatio = targetDuration / currentDuration;
    
    console.log(`\nğŸ“Š AnÃ¡lise de duraÃ§Ã£o:`);
    console.log(`   Ãudio original: ${targetDuration.toFixed(2)}s`);
    console.log(`   Ãudio TTS gerado: ${currentDuration.toFixed(2)}s`);
    console.log(`   DiferenÃ§a: ${(targetDuration - currentDuration).toFixed(2)}s`);
    console.log(`   Ratio: ${(stretchRatio * 100).toFixed(1)}%\n`);
    
    // Only stretch if we need to slow down (make longer) and difference is significant
    if (stretchRatio > 1.05 && stretchRatio <= 1.7) {
      console.log(`ğŸšï¸  Ajustando velocidade do Ã¡udio dublado...`);
      const stretchedFile = `dubbed_audio_stretched_${timestamp}.mp3`;
      
      // Use atempo to slow down the audio (inverse of speedup)
      const slowdownFactor = 1 / stretchRatio;
      await execAsync(`ffmpeg -i "${finalAudioFile}" -filter:a "atempo=${slowdownFactor}" "${stretchedFile}" -y`);
      
      // Replace original with stretched
      fs.unlinkSync(finalAudioFile);
      fs.renameSync(stretchedFile, finalAudioFile);
      
      console.log('âœ… DuraÃ§Ã£o ajustada\n');
    }
  }
  
  // Cleanup chunk files
  audioChunks.forEach(file => {
    if (fs.existsSync(file)) fs.unlinkSync(file);
  });
  if (fs.existsSync(concatListFile)) fs.unlinkSync(concatListFile);
  
  return finalAudioFile;
}

// Hybrid method: Whisper-1 for timestamps + GPT for refinement
async function transcribeWithHybridMethod(audioFile, language, duration, timestamp) {
  console.log('ğŸ”¬ MÃ©todo hÃ­brido: Whisper-1 (timestamps precisos)\n');
  
  const numChunks = Math.ceil(duration / AUDIO_CHUNK_DURATION);
  console.log(`ğŸ”ª Dividindo em ${numChunks} chunks de ~${AUDIO_CHUNK_DURATION}s cada\n`);
  
  const allSegments = [];
  const allTranscriptions = [];
  
  for (let i = 0; i < numChunks; i++) {
    const startTime = i * AUDIO_CHUNK_DURATION;
    const chunkFile = `temp_audio_chunk_${timestamp}_${i}.mp3`;
    
    console.log(`ğŸ™ï¸  Processando chunk ${i + 1}/${numChunks}...`);
    
    // Split audio using ffmpeg
    await execAsync(`ffmpeg -i "${audioFile}" -ss ${startTime} -t ${AUDIO_CHUNK_DURATION} -acodec libmp3lame -q:a 2 "${chunkFile}" -y`);
    
    // Step 1: Transcribe with Whisper-1 to get timestamps
    console.log(`ğŸ“ Transcrevendo com Whisper-1 (timestamps)...`);
    const transcription = await openai.audio.transcriptions.create({
      file: fs.createReadStream(chunkFile),
      model: 'whisper-1',
      language: language,
      response_format: 'verbose_json',
      timestamp_granularities: ['segment']
    });
    
    // Step 2: Refine text with GPT - with strict instructions to preserve ALL content
    console.log(`âœ¨ Refinando texto com gpt-5-nano...`);
    
    const charCount = transcription.text.length;
    console.log(`   Texto original: ${charCount} caracteres`);
    
    const refinementResponse = await openai.chat.completions.create({
      model: 'gpt-5-nano-2025-08-07',
      messages: [
        {
          role: 'system',
          content: `You are a transcription editor. Your job is ONLY to fix transcription errors (wrong words, typos), correct grammar, and improve readability. 

CRITICAL RULES:
- You MUST preserve EVERY single piece of information
- Do NOT summarize, condense, or shorten the text
- Do NOT skip any sentences or paragraphs
- The output must be approximately the SAME LENGTH as the input
- Only fix errors, do not rewrite or paraphrase unnecessarily
- Return ONLY the corrected text, no explanations

If the input is ${charCount} characters, your output should be around ${charCount} characters (Â±10%).`
        },
        {
          role: 'user',
          content: transcription.text
        }
      ]
    });
    
    const refinedText = refinementResponse.choices[0].message.content;
    console.log(`   Texto refinado: ${refinedText.length} caracteres (${((refinedText.length/charCount)*100).toFixed(1)}%)`);
    
    // Warn if text was significantly shortened
    if (refinedText.length < charCount * 0.85) {
      console.log(`   âš ï¸  AVISO: Texto foi reduzido em mais de 15%! Usando original.`);
      allTranscriptions.push(transcription.text);
    } else {
      allTranscriptions.push(refinedText);
    }
    
    // Store segments with adjusted timestamps
    if (transcription.segments) {
      transcription.segments.forEach(seg => {
        allSegments.push({
          start: seg.start + startTime,
          end: seg.end + startTime,
          text: seg.text
        });
      });
    }
    
    console.log(`âœ… Chunk ${i + 1}/${numChunks} processado\n`);
    
    // Cleanup chunk file
    if (fs.existsSync(chunkFile)) fs.unlinkSync(chunkFile);
  }
  
  console.log('âœ… TranscriÃ§Ã£o hÃ­brida completa\n');
  console.log(`ğŸ“Š Total de segmentos: ${allSegments.length}\n`);
  
  return {
    text: allTranscriptions.join(' '),
    duration: duration,
    segments: allSegments
  };
}

// Helper function to split audio file into chunks and transcribe
async function transcribeAudioFile(audioFile, language, timestamp, useHybridMethod = false) {
  const fileSize = fs.statSync(audioFile).size;
  
  // Get audio duration first
  const { stdout: durationInfo } = await execAsync(`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${audioFile}"`);
  const duration = parseFloat(durationInfo.trim());
  
  console.log(`ğŸ“ Tamanho do arquivo: ${(fileSize / 1024 / 1024).toFixed(2)} MB`);
  console.log(`â±ï¸  DuraÃ§Ã£o real do Ã¡udio: ${duration.toFixed(2)}s (${(duration / 60).toFixed(1)} minutos)\n`);
  
  // Force chunking for audio longer than 5 minutes (300s) to avoid incomplete transcriptions
  if (fileSize < MAX_WHISPER_FILE_SIZE && duration < 300) {
    console.log('ğŸ“ Arquivo de Ã¡udio dentro do limite, transcrevendo...\n');
    const transcription = await openai.audio.transcriptions.create({
      file: fs.createReadStream(audioFile),
      model: 'gpt-4o-mini-transcribe',
      language: language
    });
    
    const fullText = transcription.text;
    
    // Detect silence at the end using ffmpeg
    console.log('ğŸ” Detectando silÃªncios no Ã¡udio...');
    const { stdout: silenceOutput } = await execAsync(`ffmpeg -i "${audioFile}" -af silencedetect=noise=-30dB:d=0.5 -f null - 2>&1 | grep silence_end`);
    
    let lastSpeechEnd = duration;
    if (silenceOutput) {
      const silenceMatches = silenceOutput.match(/silence_end: ([\d.]+)/g);
      if (silenceMatches && silenceMatches.length > 0) {
        const lastMatch = silenceMatches[silenceMatches.length - 1];
        const lastSilenceEnd = parseFloat(lastMatch.match(/([\d.]+)/)[0]);
        // If last silence ends close to the end, assume speech ends there
        if (duration - lastSilenceEnd < 2) {
          lastSpeechEnd = lastSilenceEnd;
        }
      }
    }
    
    // Create a pseudo-segment for the end
    const pseudoSegments = [{
      start: 0,
      end: lastSpeechEnd,
      text: fullText
    }];
    
    console.log(`âœ… TranscriÃ§Ã£o completa\n`);
    
    return { 
      text: fullText, 
      duration: duration,
      segments: pseudoSegments
    };
  }
  
  console.log('âš ï¸  VÃ­deo longo detectado - usando chunking para garantir transcriÃ§Ã£o completa\n');
  
  if (useHybridMethod) {
    return await transcribeWithHybridMethod(audioFile, language, duration, timestamp);
  }
  
  // File is too large, need to split
  console.log(`âš ï¸  Arquivo de Ã¡udio grande detectado (${(fileSize / 1024 / 1024).toFixed(2)} MB)`);
  console.log('ğŸ“ Dividindo Ã¡udio em chunks para transcriÃ§Ã£o...\n');
  
  const numChunks = Math.ceil(duration / AUDIO_CHUNK_DURATION);
  console.log(`ğŸ”ª Dividindo em ${numChunks} chunks de ~${AUDIO_CHUNK_DURATION}s cada\n`);
  
  const transcriptions = [];
  const allSegments = [];
  
  for (let i = 0; i < numChunks; i++) {
    const startTime = i * AUDIO_CHUNK_DURATION;
    const chunkFile = `temp_audio_chunk_${timestamp}_${i}.mp3`;
    
    console.log(`ğŸ™ï¸  Processando chunk ${i + 1}/${numChunks}...`);
    
    // Split audio using ffmpeg
    await execAsync(`ffmpeg -i "${audioFile}" -ss ${startTime} -t ${AUDIO_CHUNK_DURATION} -acodec libmp3lame -q:a 2 "${chunkFile}" -y`);
    
    // Transcribe chunk
    console.log(`ğŸ“ Transcrevendo chunk ${i + 1}/${numChunks}...`);
    const transcription = await openai.audio.transcriptions.create({
      file: fs.createReadStream(chunkFile),
      model: 'gpt-4o-mini-transcribe',
      language: language
    });
    
    transcriptions.push(transcription.text);
    console.log(`âœ… Chunk ${i + 1}/${numChunks} transcrito`);
    
    // Cleanup chunk file
    if (fs.existsSync(chunkFile)) fs.unlinkSync(chunkFile);
  }
  
  console.log('\nâœ… Todas as transcriÃ§Ãµes completas, juntando texto...\n');
  
  // Detect silence at the end using ffmpeg on the original file
  console.log('ğŸ” Detectando silÃªncios no Ã¡udio original...');
  const { stdout: silenceOutput } = await execAsync(`ffmpeg -i "${audioFile}" -af silencedetect=noise=-30dB:d=0.5 -f null - 2>&1 | grep silence_end || echo ""`);
  
  let lastSpeechEnd = duration;
  if (silenceOutput && silenceOutput.trim()) {
    const silenceMatches = silenceOutput.match(/silence_end: ([\d.]+)/g);
    if (silenceMatches && silenceMatches.length > 0) {
      const lastMatch = silenceMatches[silenceMatches.length - 1];
      const lastSilenceEnd = parseFloat(lastMatch.match(/([\d.]+)/)[0]);
      // If last silence ends close to the end, assume speech ends there
      if (duration - lastSilenceEnd < 2) {
        lastSpeechEnd = lastSilenceEnd;
      }
    }
  }
  
  // Create a pseudo-segment for the end
  const pseudoSegments = [{
    start: 0,
    end: lastSpeechEnd,
    text: transcriptions.join(' ')
  }];
  
  return { 
    text: transcriptions.join(' '), 
    duration: duration, 
    segments: pseudoSegments
  };
}

// Language configurations
const LANGUAGES = {
  '1': { code: 'pt', name: 'PortuguÃªs ğŸ‡§ğŸ‡·', systemPrompt: 'Brazilian Portuguese' },
  '2': { code: 'en', name: 'English ğŸ‡ºğŸ‡¸', systemPrompt: 'English' },
  '3': { code: 'es', name: 'EspaÃ±ol ğŸ‡ªğŸ‡¸', systemPrompt: 'Spanish' },
  '4': { code: 'fr', name: 'FranÃ§ais ğŸ‡«ğŸ‡·', systemPrompt: 'French' },
  '5': { code: 'de', name: 'Deutsch ğŸ‡©ğŸ‡ª', systemPrompt: 'German' },
  '6': { code: 'it', name: 'Italiano ğŸ‡®ğŸ‡¹', systemPrompt: 'Italian' },
  '7': { code: 'ja', name: 'æ—¥æœ¬èª ğŸ‡¯ğŸ‡µ', systemPrompt: 'Japanese' },
  '8': { code: 'ko', name: 'í•œêµ­ì–´ ğŸ‡°ğŸ‡·', systemPrompt: 'Korean' },
  '9': { code: 'zh', name: 'ä¸­æ–‡ ğŸ‡¨ğŸ‡³', systemPrompt: 'Chinese' }
};

// Voice options
const VOICES = {
  '1': { id: 'alloy', name: 'Alloy ğŸµ (Neutro e equilibrado)' },
  '2': { id: 'echo', name: 'Echo ğŸ™ï¸ (Masculino e claro)' },
  '3': { id: 'fable', name: 'Fable âœ¨ (Expressivo e animado)' },
  '4': { id: 'onyx', name: 'Onyx ğŸ¬ (Masculino profundo)' },
  '5': { id: 'nova', name: 'Nova ğŸŒŸ (Feminino jovem)' },
  '6': { id: 'shimmer', name: 'Shimmer ğŸ’« (Feminino suave)' }
};

// Quality options for YouTube download
const QUALITY_OPTIONS = {
  '1': { name: 'Original ğŸŒŸ (Melhor qualidade disponÃ­vel)', format: 'bestvideo+bestaudio/best' },
  '2': { name: '1080p ğŸ“º (Full HD)', format: 'bestvideo[height<=1080]+bestaudio/best[height<=1080]' },
  '3': { name: '720p ğŸ’» (HD)', format: 'bestvideo[height<=720]+bestaudio/best[height<=720]' },
  '4': { name: '480p ğŸ“± (SD)', format: 'bestvideo[height<=480]+bestaudio/best[height<=480]' }
};

async function downloadYouTubeVideo(url, formatOption) {
  console.log('\nğŸš€ Iniciando download do YouTube...\n');

  const args = [
    url,
    '-f', formatOption,
    '--merge-output-format', 'mp4',
    '-N', '10',
    '--progress',
    '--newline',
    '-o', 'downloads/%(title)s.%(ext)s'
  ];

  // Create downloads folder if it doesn't exist
  if (!fs.existsSync('downloads')) {
    fs.mkdirSync('downloads');
  }

  const ytdlp = spawn('yt-dlp', args);

  let outputFile = '';

  ytdlp.stdout.on('data', (data) => {
    const output = data.toString();
    process.stdout.write(output);
    
    // Capture the output filename from multiple patterns
    let match = output.match(/\[Merger\] Merging formats into "(.+?)"/);
    if (match) {
      outputFile = match[1];
    }
    
    // Also check for "already downloaded" message
    match = output.match(/\[download\] (.+?) has already been downloaded/);
    if (match) {
      outputFile = match[1];
    }
    
    // Also check for destination pattern
    match = output.match(/\[download\] Destination: (.+?)$/m);
    if (match) {
      outputFile = match[1];
    }
  });

  ytdlp.stderr.on('data', (data) => {
    const output = data.toString();
    // Only show errors, not info messages
    if (output.includes('ERROR')) {
      process.stderr.write(output);
    }
  });

  return new Promise((resolve, reject) => {
    ytdlp.on('close', (code) => {
      if (code === 0) {
        console.log('\nâœ… Download concluÃ­do!\n');
        
        // If we couldn't capture the filename, find the most recent .mp4 file
        if (!outputFile) {
          const files = fs.readdirSync('downloads')
            .filter(file => file.endsWith('.mp4'))
            .map(file => ({
              name: file,
              path: `downloads/${file}`,
              time: fs.statSync(`downloads/${file}`).mtime.getTime()
            }))
            .sort((a, b) => b.time - a.time); // Sort by most recent first
          
          if (files.length > 0) {
            outputFile = files[0].path;
            console.log(`ğŸ“ Arquivo mais recente detectado: ${files[0].name}`);
          }
        }
        
        resolve(outputFile);
      } else {
        reject(new Error(`Download falhou com cÃ³digo ${code}`));
      }
    });
  });
}

async function dubVideo(inputVideo, sourceLang, targetLang, voiceId, askConfirmation = true, useHybridMethod = false) {
  console.log('\nğŸ¬ Iniciando processo de dublagem...\n');
  console.log(`ğŸ“¹ VÃ­deo de entrada: ${inputVideo}`);
  console.log(`ğŸ—£ï¸  ${sourceLang.name} â†’ ${targetLang.name}\n`);

  const timestamp = Date.now();
  const audioFile = `temp_audio_${timestamp}.mp3`;
  const dubbedAudioFile = `dubbed_audio_${timestamp}.mp3`;
  const outputVideo = inputVideo.replace('.mp4', `_${targetLang.code}.mp4`);

  try {
    // Step 1: Extract audio from video
    console.log('ğŸ“¤ Extraindo Ã¡udio do vÃ­deo...');
    await execAsync(`ffmpeg -i "${inputVideo}" -vn -acodec libmp3lame -q:a 2 "${audioFile}" -y`);
    console.log('âœ… Ãudio extraÃ­do\n');

    // Step 2: Transcribe audio to text (with chunking for large files)
    console.log(`ğŸ™ï¸  Transcrevendo Ã¡udio em ${sourceLang.name}...`);
    const transcriptionResult = await transcribeAudioFile(audioFile, sourceLang.code, timestamp, useHybridMethod);
    const transcriptionText = transcriptionResult.text;
    const originalAudioDuration = transcriptionResult.duration;
    const segments = transcriptionResult.segments;
    
    console.log('âœ… TranscriÃ§Ã£o completa:', transcriptionText.substring(0, 150) + '...\n');
    console.log(`â±ï¸  DuraÃ§Ã£o do Ã¡udio original: ${originalAudioDuration.toFixed(2)}s`);
    
    if (segments && segments.length > 0) {
      const lastSegmentEnd = segments[segments.length - 1].end;
      const silenceAtEnd = originalAudioDuration - lastSegmentEnd;
      console.log(`ğŸ”Š Ãšltima fala termina em: ${lastSegmentEnd.toFixed(2)}s`);
      console.log(`ğŸ”‡ SilÃªncio no final: ${silenceAtEnd.toFixed(2)}s\n`);
    } else {
      console.log('');
    }

    // Step 3: Translate text
    console.log(`ğŸŒ Traduzindo para ${targetLang.name}...`);
    const translationResponse = await openai.chat.completions.create({
      model: 'o4-mini',
      messages: [
        {
          role: 'system',
          content: `You are a professional translator. Translate the following text from ${sourceLang.systemPrompt} to ${targetLang.systemPrompt}. Keep the same tone, style, and natural flow. Only return the translated text, nothing else.`
        },
        {
          role: 'user',
          content: transcriptionText
        }
      ]
    });
    const translatedText = translationResponse.choices[0].message.content;
    console.log('âœ… TraduÃ§Ã£o:', translatedText.substring(0, 150) + '...\n');
    
    // Save transcription and translation for debugging
    const debugFolder = 'debug_logs';
    if (!fs.existsSync(debugFolder)) {
      fs.mkdirSync(debugFolder);
    }
    
    fs.writeFileSync(`${debugFolder}/transcription_${timestamp}.txt`, transcriptionText);
    fs.writeFileSync(`${debugFolder}/translation_${timestamp}.txt`, translatedText);
    
    const transcriptionWords = transcriptionText.split(/\s+/).length;
    const translationWords = translatedText.split(/\s+/).length;
    
    console.log(`ğŸ“Š EstatÃ­sticas:`);
    console.log(`   Original: ${transcriptionText.length} caracteres, ${transcriptionWords} palavras`);
    console.log(`   TraduÃ§Ã£o: ${translatedText.length} caracteres, ${translationWords} palavras`);
    console.log(`   Ratio: ${(translationWords / transcriptionWords * 100).toFixed(1)}%\n`);

    // Ask if user wants to continue with this translation (only if askConfirmation is true)
    if (askConfirmation) {
      const continueChoice = await question('ğŸ“‹ Deseja continuar com esta traduÃ§Ã£o? (s/n): ');
      if (continueChoice.toLowerCase() !== 's') {
        console.log('â¸ï¸  Processo cancelado pelo usuÃ¡rio.');
        cleanup();
        return null;
      }
    } else {
      console.log('â–¶ï¸  Continuando automaticamente...\n');
    }

    // Step 4: Generate speech using TTS (with chunking for long texts)
    console.log(`ğŸ”Š Gerando Ã¡udio dublado com voz ${voiceId}...`);
    
    // Calculate target duration (exclude trailing silence if we have segments)
    let targetDuration = originalAudioDuration;
    if (segments && segments.length > 0) {
      const lastSegmentEnd = segments[segments.length - 1].end;
      targetDuration = lastSegmentEnd; // Only match duration up to last speech
      console.log(`ğŸ¯ Ajustando para duraÃ§Ã£o da fala: ${targetDuration.toFixed(2)}s (excluindo silÃªncio final)\n`);
    }
    
    let finalAudioPath;
    if (translatedText.length > MAX_TTS_CHARS) {
      console.log(`âš ï¸  Texto longo detectado (${translatedText.length} caracteres)\n`);
      finalAudioPath = await generateTTSForChunks(translatedText, voiceId, timestamp, targetDuration);
      
      if (!finalAudioPath) {
        // Fallback to single TTS if chunking returned null
        const speechResponse = await openai.audio.speech.create({
          model: 'gpt-4o-mini-tts',
          voice: voiceId,
          input: translatedText,
        });
        const buffer = Buffer.from(await speechResponse.arrayBuffer());
        fs.writeFileSync(dubbedAudioFile, buffer);
        finalAudioPath = dubbedAudioFile;
      }
    } else {
      // Normal single TTS for short texts
      console.log('ğŸ“ Gerando Ã¡udio em uma Ãºnica requisiÃ§Ã£o\n');
      const speechResponse = await openai.audio.speech.create({
        model: 'gpt-4o-mini-tts',
        voice: voiceId,
        input: translatedText,
      });
      const buffer = Buffer.from(await speechResponse.arrayBuffer());
      fs.writeFileSync(dubbedAudioFile, buffer);
      finalAudioPath = dubbedAudioFile;
    }
    
    // Check final audio duration and pad with silence to match video
    const { stdout: currentDurationInfo } = await execAsync(`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${finalAudioPath}"`);
    const currentAudioDuration = parseFloat(currentDurationInfo.trim());
    
    console.log(`\nğŸ“ DuraÃ§Ã£o atual do Ã¡udio: ${currentAudioDuration.toFixed(2)}s`);
    console.log(`ğŸ“ DuraÃ§Ã£o do vÃ­deo: ${originalAudioDuration.toFixed(2)}s`);
    
    const silenceNeeded = originalAudioDuration - currentAudioDuration;
    
    if (silenceNeeded > 0.5) {
      console.log(`ğŸ”‡ Adicionando ${silenceNeeded.toFixed(2)}s de silÃªncio para igualar duraÃ§Ã£o do vÃ­deo...`);
      const finalWithSilence = `dubbed_audio_with_silence_${timestamp}.mp3`;
      await execAsync(`ffmpeg -i "${finalAudioPath}" -f lavfi -t ${silenceNeeded} -i anullsrc=r=44100:cl=stereo -filter_complex "[0:a][1:a]concat=n=2:v=0:a=1" "${finalWithSilence}" -y`);
      
      if (finalAudioPath !== dubbedAudioFile) {
        fs.unlinkSync(finalAudioPath);
      }
      finalAudioPath = finalWithSilence;
      console.log('âœ… SilÃªncio adicionado\n');
    } else {
      console.log(`âœ… DuraÃ§Ã£o jÃ¡ estÃ¡ correta\n`);
    }
    
    console.log('âœ… Ãudio dublado gerado\n');

    // Step 5: Check duration and adjust if needed
    console.log('â±ï¸  Verificando duraÃ§Ã£o do vÃ­deo/Ã¡udio...');
    const { stdout: videoInfo } = await execAsync(`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${inputVideo}"`);
    const videoDuration = parseFloat(videoInfo.trim());

    const { stdout: audioInfo } = await execAsync(`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${finalAudioPath}"`);
    const audioDuration = parseFloat(audioInfo.trim());

    console.log(`ğŸ“¹ VÃ­deo: ${videoDuration.toFixed(2)}s`);
    console.log(`ğŸµ Ãudio: ${audioDuration.toFixed(2)}s\n`);

    // Step 6: Replace audio in video
    console.log('ğŸ¥ Substituindo Ã¡udio no vÃ­deo...');

    let audioFilter = '';
    const speedRatio = videoDuration / audioDuration;

    // atempo only supports 0.5 to 2.0, for larger changes we need to chain multiple atempo filters
    if (Math.abs(speedRatio - 1) > 0.05) {
      if (speedRatio >= 0.5 && speedRatio <= 2.0) {
        console.log(`âš™ï¸  Ajustando velocidade do Ã¡udio em ${(speedRatio * 100).toFixed(1)}%...`);
        audioFilter = `-filter:a "atempo=${speedRatio}"`;
      } else if (speedRatio > 2.0) {
        // Chain multiple atempo for speed > 2x
        console.log(`âš™ï¸  Ajustando velocidade do Ã¡udio em ${(speedRatio * 100).toFixed(1)}% (cadeia mÃºltipla)...`);
        const iterations = Math.ceil(Math.log2(speedRatio));
        let filters = [];
        let remaining = speedRatio;
        for (let i = 0; i < iterations; i++) {
          const step = Math.min(remaining, 2.0);
          filters.push(`atempo=${step}`);
          remaining /= step;
        }
        audioFilter = `-filter:a "${filters.join(',')}"`;
      } else {
        // For very slow speeds, also chain
        console.log(`âš™ï¸  Ajustando velocidade do Ã¡udio em ${(speedRatio * 100).toFixed(1)}% (cadeia mÃºltipla)...`);
        const iterations = Math.ceil(Math.log2(1/speedRatio));
        let filters = [];
        let remaining = speedRatio;
        for (let i = 0; i < iterations; i++) {
          const step = Math.max(remaining, 0.5);
          filters.push(`atempo=${step}`);
          remaining /= step;
        }
        audioFilter = `-filter:a "${filters.join(',')}"`;
      }
    }

    await execAsync(`ffmpeg -i "${inputVideo}" -i "${finalAudioPath}" -c:v copy ${audioFilter} -map 0:v:0 -map 1:a:0 "${outputVideo}" -y`);
    console.log('âœ… VÃ­deo dublado criado!\n');

    // Cleanup
    console.log('ğŸ§¹ Limpando arquivos temporÃ¡rios...');
    cleanup();
    console.log('âœ… Limpeza concluÃ­da\n');

    console.log(`ğŸ‰ PRONTO! Seu vÃ­deo dublado estÃ¡ aqui: ${outputVideo}`);
    return outputVideo;

  } catch (error) {
    console.error('\nâŒ Erro durante a dublagem:', error.message);
    cleanup();
    throw error;
  }

  function cleanup() {
    try {
      if (fs.existsSync(audioFile)) fs.unlinkSync(audioFile);
      if (fs.existsSync(dubbedAudioFile)) fs.unlinkSync(dubbedAudioFile);
    } catch (e) {
      console.error('âš ï¸  Aviso: NÃ£o foi possÃ­vel limpar todos os arquivos temporÃ¡rios');
    }
  }
}

async function selectLanguage(prompt) {
  console.log(prompt);
  Object.entries(LANGUAGES).forEach(([key, { name }]) => {
    console.log(`  ${key}. ${name}`);
  });

  const choice = await question('\nğŸ”¢ Digite o nÃºmero da opÃ§Ã£o: ');
  const selected = LANGUAGES[choice];

  if (!selected) {
    console.log('âŒ OpÃ§Ã£o invÃ¡lida!');
    return null;
  }

  console.log(`âœ¨ Selecionado: ${selected.name}\n`);
  return selected;
}

async function selectVoice() {
  console.log('\nğŸ¤ Escolha a voz para dublagem:\n');
  Object.entries(VOICES).forEach(([key, { name }]) => {
    console.log(`  ${key}. ${name}`);
  });

  const choice = await question('\nğŸ”¢ Digite o nÃºmero da opÃ§Ã£o: ');
  const selected = VOICES[choice];

  if (!selected) {
    console.log('âŒ OpÃ§Ã£o invÃ¡lida!');
    return null;
  }

  console.log(`âœ¨ Voz selecionada: ${selected.name}\n`);
  return selected.id;
}

async function selectQuality() {
  console.log('\nğŸ“Š Escolha a qualidade do download:\n');
  Object.entries(QUALITY_OPTIONS).forEach(([key, { name }]) => {
    console.log(`  ${key}. ${name}`);
  });

  const choice = await question('\nğŸ”¢ Digite o nÃºmero da opÃ§Ã£o: ');
  const selected = QUALITY_OPTIONS[choice];

  if (!selected) {
    console.log('âŒ OpÃ§Ã£o invÃ¡lida!');
    return null;
  }

  console.log(`âœ¨ Qualidade selecionada: ${selected.name}\n`);
  return selected.format;
}

async function main() {
  console.log('\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
  console.log('â•‘   ğŸ¬ AI VIDEO DUBBING STUDIO ğŸ™ï¸       â•‘');
  console.log('â•‘   Baixe e Duble VÃ­deos com IA! âœ¨     â•‘');
  console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

  console.log('ğŸ¯ O que vocÃª deseja fazer?\n');
  console.log('  1. ğŸŒ Baixar vÃ­deo do YouTube e dublar');
  console.log('  2. ğŸ“ Dublar um vÃ­deo local existente');
  console.log('  3. ğŸšª Sair\n');

  const mainChoice = await question('ğŸ”¢ Digite o nÃºmero da opÃ§Ã£o: ');

  let videoFile = '';

  if (mainChoice === '1') {
    // Download from YouTube
    console.log('\nğŸŒ === DOWNLOAD DO YOUTUBE ===\n');
    
    const url = await question('ğŸ“ Cole a URL do vÃ­deo do YouTube: ');
    
    if (!url.includes('youtube.com') && !url.includes('youtu.be')) {
      console.log('âŒ URL invÃ¡lida! Use uma URL do YouTube.');
      rl.close();
      return;
    }

    const quality = await selectQuality();
    if (!quality) {
      rl.close();
      return;
    }

    try {
      videoFile = await downloadYouTubeVideo(url, quality);
      console.log(`ğŸ“¹ VÃ­deo baixado: ${videoFile}\n`);
    } catch (error) {
      console.error('âŒ Erro no download:', error.message);
      rl.close();
      return;
    }

  } else if (mainChoice === '2') {
    // Use existing video
    console.log('\nğŸ“ === VÃDEO LOCAL ===\n');
    videoFile = await question('ğŸ“‚ Cole o caminho do arquivo de vÃ­deo (ou arraste aqui): ');
    videoFile = videoFile.replace(/['"]/g, '').trim();

    if (!fs.existsSync(videoFile)) {
      console.log('âŒ Arquivo nÃ£o encontrado!');
      rl.close();
      return;
    }

  } else if (mainChoice === '3') {
    console.log('\nğŸ‘‹ AtÃ© logo!\n');
    rl.close();
    return;
  } else {
    console.log('âŒ OpÃ§Ã£o invÃ¡lida!');
    rl.close();
    return;
  }

  // Dubbing process
  console.log('\nğŸ™ï¸  === CONFIGURAÃ‡ÃƒO DA DUBLAGEM ===\n');

  const sourceLang = await selectLanguage('ğŸ—£ï¸  Idioma ORIGINAL do vÃ­deo:\n');
  if (!sourceLang) {
    rl.close();
    return;
  }

  const targetLang = await selectLanguage('ğŸ¯ Idioma ALVO (para qual deseja dublar):\n');
  if (!targetLang) {
    rl.close();
    return;
  }

  const voiceId = await selectVoice();
  if (!voiceId) {
    rl.close();
    return;
  }

  // Select transcription method
  console.log('\nğŸ”¬ MÃ©todo de transcriÃ§Ã£o:\n');
  console.log('  1. RÃ¡pido e EconÃ´mico (gpt-4o-mini + detecÃ§Ã£o de silÃªncio)');
  console.log('  2. Preciso com Timestamps (whisper-1 com timestamps exatos)\n');
  
  const transcriptionChoice = await question('ğŸ”¢ Escolha o mÃ©todo (Enter=RÃ¡pido): ');
  const useHybridMethod = transcriptionChoice === '2';
  
  if (useHybridMethod) {
    console.log('âœ¨ Usando mÃ©todo com timestamps: Whisper-1\n');
  } else {
    console.log('âš¡ Usando mÃ©todo rÃ¡pido: gpt-4o-mini + ffmpeg\n');
  }

  const confirmChoice = await question('ğŸ’¡ Deseja revisar a traduÃ§Ã£o antes de gerar o Ã¡udio? (s/n): ');
  const askConfirmation = confirmChoice.toLowerCase() === 's';

  rl.close();

  // Start dubbing
  try {
    await dubVideo(videoFile, sourceLang, targetLang, voiceId, askConfirmation, useHybridMethod);
    console.log('\nğŸŒŸ Processo concluÃ­do com sucesso! ğŸŒŸ\n');
  } catch (error) {
    console.error('\nâŒ Erro:', error.message);
    process.exit(1);
  }
}

main();
